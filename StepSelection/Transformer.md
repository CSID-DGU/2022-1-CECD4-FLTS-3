# Transformer
OXAI 제공 링크 : http://jalammar.github.io/illustrated-transformer/

seq2seq모델의 한계 -> Attention Mechanism -> Transformer -> stage2 of Deep Saber

## seq2seq모델


어떤 시퀀스 데이터를 다른 도메인에 있는 시퀀스 데이터로 변환하고자 할 때 쓰는 모델

보통 챗봇이나 기계 번역에 쓰임


두개의 모듈로 분리되고 각각 RNN 셀들로 구성되어 있음 -> 두 개의 RNN 아키텍처라고 볼 수 있다.

- 인코더 : 입력한 시퀀스의 데이터를 순차적으로 입력받고 모든 데이터 정보들을 압축해서 하나의 Context Vector를 만들어 디코더에게 전달

- 디코더 : 테스트 과정과 훈련과정이 다름

테스트 과정 - 컨텍스트 벡터와 시작을 의미하는 심볼 <sos>를 받아서 변환된 데이터를 순차적으로 출력, 다음에 올 데이터를 예측하고, 예측한 데이터를 다음 시점의 RNN 셀로 입력하는 것을 끝을 의미하는 심볼 <eos>가 예측될 때까지 반복

훈련 과정 - 컨텍스트 벡터와 실제 정답, 예를 들어 <sos> a b c 를 입력받고, 정답으로 a b c <eos>가 나와야한다고 정답을 알려주면서 훈련


시퀀스의 각 데이터를 벡터로 바꿀때 임베딩을 사용한다. 각각의 임베딩 벡터가 있는 것


인코더의 RNN 셀 하나을 볼 때 그 시점이 t라고 하면,

t-1에서의 은닉상태와 t에서의 임베딩 벡터를 입력 받고

t에서의 은닉상태를 출력함


이때 은닉 상태 -> 과거 시점의 동일한 RNN 셀에서의 모든 은닉상태의 값들의 영향을 누적해서 받은 것

인코더의 마지막 셀의 은닉 상태 = 컨텍스트 벡터 = 입력 시퀀스 내의 데이터들에 대한 정보 요약

...

https://wikidocs.net/24996


## Attention Mechanism

신경망들의 성능을 높이기 위한 메커니즘.


seq2seq의 한계

1. 하나의 벡터로 압축 -> 정보의 손실이 발생

2. RNN의 문제 -> Vanishing Gradient


입력 시퀀스가 길어질 수록 품질이 떨어진다. 정확도 보정의 기법이 어텐션


디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서의 전체 입력 문장을 다시 참고하는데,

해당 시점에서 예측해야할 데이터와 연관이 있는 입력 데이터 부분을 더 집중해서 봄


어텐션 함수는 다음과 같이 간단히 표현할 수 있음

~~~
Attention(Q, K, V) = Attention Value

seq2seq + Attention 모델에서 의미
Q = Query : t 시점의 디코더 셀에서의 은닉 상태
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
~~~

어떤 Query에 대해서 모든 Key와의 유사도를 구함 (Soft max)

이 유사도를 Key에 해당하는 Value에 반영

Value들을 모두 더해서 리턴 -> Attention Value

...

https://wikidocs.net/22893

